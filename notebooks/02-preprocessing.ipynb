{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Energy Predictor - Preprocessing\n",
    "#### Hosted by: ASHRAE\n",
    "##### Source: https://www.kaggle.com/c/ashrae-energy-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section I: Dependencies and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import src.utils as udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "sns.set(rc={'figure.figsize': (16, 4),\n",
    "            'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_path = '../data/from_eda/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20138871 entries, 0 to 20216099\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   building_id    uint16        \n",
      " 1   meter          uint8         \n",
      " 2   timestamp      datetime64[ns]\n",
      " 3   meter_reading  float32       \n",
      " 4   site_id        uint8         \n",
      " 5   meter_type     object        \n",
      " 6   dayofyear      uint16        \n",
      " 7   month          uint8         \n",
      " 8   day            uint8         \n",
      " 9   dayofweek      uint8         \n",
      " 10  hour           uint8         \n",
      "dtypes: datetime64[ns](1), float32(1), object(1), uint16(2), uint8(6)\n",
      "memory usage: 729.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Meter data\n",
    "meter = pd.read_pickle(f'{data_path}meter.pkl')\n",
    "meter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139773 entries, 0 to 139772\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   site_id             139773 non-null  uint8         \n",
      " 1   timestamp           139773 non-null  datetime64[ns]\n",
      " 2   air_temperature     139718 non-null  float32       \n",
      " 3   cloud_coverage      70600 non-null   float32       \n",
      " 4   dew_temperature     139660 non-null  float32       \n",
      " 5   precip_depth_1_hr   89484 non-null   float32       \n",
      " 6   sea_level_pressure  129155 non-null  float32       \n",
      " 7   wind_direction      133505 non-null  float32       \n",
      " 8   wind_speed          139469 non-null  float32       \n",
      "dtypes: datetime64[ns](1), float32(7), uint8(1)\n",
      "memory usage: 4.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Weather data\n",
    "weather = pd.read_pickle(f'{data_path}weather.pkl')\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1449 entries, 0 to 1448\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   site_id      1449 non-null   uint8  \n",
      " 1   building_id  1449 non-null   uint16 \n",
      " 2   primary_use  1449 non-null   object \n",
      " 3   square_feet  1449 non-null   uint32 \n",
      " 4   year_built   675 non-null    float32\n",
      " 5   floor_count  355 non-null    float32\n",
      "dtypes: float32(2), object(1), uint16(1), uint32(1), uint8(1)\n",
      "memory usage: 32.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Building data\n",
    "building = pd.read_pickle(f'{data_path}building.pkl')\n",
    "building.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data_path\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section II: Meter Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values and zero values in meter data\n",
    "\n",
    "Aside from the missing meter readings, there are a lot of zero-readings. In the last notebook (`01-eda.ipynb`), we saw that site 0 buildings had readings of 0 for the first 5 months. It is a lot more likely that zero-readings are errors in the data than actually readings of 0, so we will inspect the readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values and zeroes\n",
    "# Adapted from: https://www.kaggle.com/hmendonca/clean-weather-data-eda#Check-the-meter-averages-per-weekday\n",
    "\n",
    "meter_types = ['electricity', 'chilledwater', 'steam', 'hotwater']\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 30))\n",
    "\n",
    "for m in range(4):\n",
    "    \n",
    "    df = meter[meter.meter == m]\n",
    "    mmap = df.pivot(index='building_id', columns='timestamp', values='meter_reading')\n",
    "    mmap = mmap.reindex(sorted(meter.building_id.unique()))\n",
    "    mmap = np.sign(mmap)\n",
    "    \n",
    "    sns.heatmap(mmap, cmap='Paired', ax=ax[m], cbar=False)\n",
    "    ax[m].set_title(f'{meter_types[m]} meter')\n",
    "    ax[m].set_ylabel(None)\n",
    "    ax[m].set_xlabel(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was adapted from a Kaggle user (source in the comment above). For clarity, the plot description is as follows:\n",
    "- Each of the 4 subplots represents the meter readings from a particular meter type\n",
    "    - From left to right: `electricity`, `chilledwater`, `steam`, `hotwater`\n",
    "- Y-axis: building number\n",
    "- X-axis: 2016 timestamp\n",
    "- Orange represents non-zero meter readings\n",
    "- Blue represents zero-readings\n",
    "- No color represents no readings\n",
    "\n",
    "Patterns in the meter readings:\n",
    "- Site 0 buildings are missing readings for both `electricity` and `chilledwater` meters at the beginning of the year\n",
    "- Site 15 buildings are missing a chunk of readings between February and March for all their meters: `electricity`, `chilledwater`, and `steam`\n",
    "- A lot of the blue shading come in streaks of long periods of time, so these may actually be missing readings\n",
    "\n",
    "The most efficient course of action here would be to just remove these zero-readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop observations with a meter reading of 0\n",
    "meter.drop(index=meter[meter.meter_reading <= 0].index, inplace=True)\n",
    "meter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buildings built in 2016 and 2017\n",
    "\n",
    "The data contains buildings with a `year_built` value of 2016 and 2017. This is strange as the meter data contains readings from 2016. It's possible that the readings from these buildings are actually the energy consumption during construction. Either way, these buildings are not aligned with our purpose of predicting energy consumption in normal conditions, so we will inspect these buildings and most likely drop them from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect buildings with a year_built of 2016 and 2017\n",
    "yr_built_16_17 = building[building.year_built.isin([2016, 2017])]\n",
    "yr_built_16_17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count meter readings in these buildings\n",
    "n_readings_16_17 = meter[meter.building_id.isin(yr_built_16_17.building_id)] # Filter for 2016/2017 buildings\n",
    "n_readings_16_17 = n_readings_16_17.groupby('building_id').meter_reading.count() # Count readings for each building\n",
    "n_readings_16_17['Total'] = n_readings_16_17.sum() # Calculate total\n",
    "pd.DataFrame(n_readings_16_17).reset_index() # Convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost 60 thousand meter readings from buildings built in either 2016 or 2017. That's quite a bit of meter readings, but relatively insignificant when we consider that there are over 18 million readings in the data. We will drop these buildings and their meter readings from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 2016 and 2017 buildings\n",
    "meter = meter[~meter.building_id.isin(yr_built_16_17.building_id)]\n",
    "meter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del meter_types, fig, ax, m, df, mmap, yr_built_16_17, n_readings_16_17\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section III: Weather Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindex weather data\n",
    "\n",
    "In the last notebook, we saw that the weather data for 2016 was incomplete, i.e. it did not have a record for every hour of every day in every site (24 hours, 366 days in 2016, 16 sites). In other words, there should be 24 x 366 x 16, or `140,544`, timestamps, but more than 700 were missing. So we will reindex the weather data to include every timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex weather data to include every hour of every day in 2016\n",
    "weather = udf.reidx_site_time(weather, t_start='1/1/2016 00:00:00', t_end='12/31/2016 23:00:00')\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "wthr_missing = pd.DataFrame(zip(weather.isnull().sum(), np.round(weather.isnull().mean() * 100, 2)), \n",
    "                            index=weather.columns, columns=['missing', 'pct_missing'])\n",
    "wthr_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values by site\n",
    "udf.missing_vals_by_site(weather, pct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of missing `cloud_coverage` and `precip_depth_1_hr` values in the data, with a couple of sites even missing 100% of these values. Filling in this much data may introduce a large bias to the data, so we will be dropping these weather features instead.\n",
    "\n",
    "All of the other weather features have less than 10% of their values missing, so filling their values wouldn't have as much of an adverse effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop cloud_coverage and precip_depth_1_hr\n",
    "weather.drop(['cloud_coverage', 'precip_depth_1_hr'], axis=1, inplace=True)\n",
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time interval to plot\n",
    "start, end = datetime.date(2016, 1, 1), datetime.date(2016, 1, 31)\n",
    "\n",
    "# Site 15 air temperature and wind speed\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "w15 = udf.get_site(weather, 15, time_idx=True)\n",
    "w15.loc[start:end, ['air_temperature', 'wind_speed']].plot(subplots=True, ax=ax, xticks=[], xlabel='Day', \n",
    "                                                           title='January 2016 Weather (with missing values)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use linear interpolation for variables with min/max constraints and cubic interpolation for the rest\n",
    "lin_cols = ['wind_direction', 'wind_speed']\n",
    "cub_cols = ['air_temperature', 'dew_temperature', 'sea_level_pressure']\n",
    "\n",
    "# Fill missing values by site\n",
    "weather = udf.fill_missing(weather, [], lin_cols, cub_cols)\n",
    "udf.missing_vals_by_site(weather, pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site 15 air temperature and wind speed\n",
    "fig, ax = plt.subplots(2, 1, figsize=(16, 8))\n",
    "w15 = udf.get_site(weather, 15, time_idx=True)\n",
    "w15.loc[start:end, ['air_temperature', 'wind_speed']].plot(subplots=True, ax=ax, xticks=[], xlabel='Day', \n",
    "                                                           title='January 2016 Weather')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic interpolation\n",
    "I decided to use cubic interpolation because using a polynomial higher than 1 would result in a much smoother estimation of the missing data, as seen in the plot above. Of course, this wasn't possible for every feature due to value constraints, so it was only used for `air_temperature`, `dew_temperature`, and `sea_level_pressure`.\n",
    "\n",
    "##### Linear interpolation\n",
    "Since `wind_direction` is limited to values between 0 and 360, and `wind_speed` is limited to non-negative values, linear interpolation was a better choice because it wouldn't result in values outside of these constraints.\n",
    "\n",
    "##### Forward/backward fill\n",
    "As interpolation only works for missing values between non-missing values, any leading or trailing missing values (at the beginning or end of the year) was filled using a forward/backward fill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing `sea_level_pressure` values in site 5\n",
    "\n",
    "The data imputation used above was done on a per-site basis, which means if a site was missing all of its values for a certain feature, this method wouldn't work. While we did manage to fill in most of the missing weather data, site 5 was missing 100% of the `sea_level_pressure` values so these values remain missing.\n",
    "\n",
    "From the EDA in the last notebook, we discovered that site 5 is in Europe. Because sites 1 and 12 are also in Europe, they may share similar weather to site 5. Without much other context, copying the `sea_level_pressure` from either site 1 or 12 over to site 5 may be our best estimation for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 3 European sites\n",
    "w1 = udf.get_site(weather, 1, time_idx=True)\n",
    "w5 = udf.get_site(weather, 5, time_idx=True)\n",
    "w12 = udf.get_site(weather, 12, time_idx=True)\n",
    "\n",
    "# Air temperature in January from all 3\n",
    "w1.loc[start:end, 'air_temperature'].plot(label='Site 1')\n",
    "w5.loc[start:end, 'air_temperature'].plot(label='Site 5')\n",
    "w12.loc[start:end, 'air_temperature'].plot(label='Site 12')\n",
    "\n",
    "# Formatting\n",
    "plt.title('January 2016 Temperature of European Sites')\n",
    "plt.ylabel('Air temperature')\n",
    "plt.xlabel('Day')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sea-level pressure in January from sites \n",
    "w1.loc[start:end, 'sea_level_pressure'].plot(label='Site 1')\n",
    "w12.loc[start:end, 'sea_level_pressure'].plot(label='Site 12')\n",
    "plt.title('January 2016 Sea-level Pressure of Sites 1 and 12')\n",
    "plt.ylabel('Sea level pressure')\n",
    "plt.xlabel('Day')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the weather of site 5 is more similar to site 1 than site 12, so we will be using the `sea_level_pressure` from site 1 to copy over to site 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of sites 1 and 5\n",
    "i1 = weather[weather.site_id == 1].index \n",
    "i5 = weather[weather.site_id == 5].index\n",
    "\n",
    "# Copy site 1's sea_level_pressure data over to site 5\n",
    "weather.loc[i5, 'sea_level_pressure'] = weather.loc[i1, 'sea_level_pressure'].values\n",
    "weather[weather.site_id == 5].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect missing values again\n",
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recast wind direction to integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wind_direction to unsigned integers\n",
    "weather['wind_direction'] = weather.wind_direction.astype('uint16')\n",
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wthr_missing, start, end, w15, lin_cols, cub_cols, w1, w5, w12, i1, i5, fig, ax\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section IV: Building Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "bldg_missing = pd.DataFrame(zip(building.isnull().sum(), round(building.isnull().mean() * 100, 2)), \n",
    "                            index=building.columns, columns=['missing', 'pct_missing'])\n",
    "bldg_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature correlation\n",
    "building.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with more than 75% of its values missing, `floor_count` has a moderate linear correlation with `square_feet`. It certainly makes sense, since both of these features are describing the size of the building. We will drop `floor_count` because of the redundancy and since it's missing most of its values anyway.\n",
    "\n",
    "`Year_built`, on the other hand, does seem to be providing information that the other features are not, so it may be useful to keep this feature. We will be imputing the missing `year_built` values with the median because it's more intuitive to fill a year value with an integer statistic and the median is more robust to outliers than the mean. \n",
    "\n",
    "Instead of using the overall median, each building's missing `year_built` value will be imputed with its site's median `year_built`. This would be a better estimation since buildings in the same area are likely to have been built around the same time. But imputing more than half the data means we're guessing on quite a bit of data, so let's first create a boolean missing indicator for `year_built` so that we'll know downstream which values were imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop floor_count\n",
    "building.drop('floor_count', axis=1, inplace=True)\n",
    "\n",
    "# Add missing indicator for year_built\n",
    "building['missing_year'] = building.year_built.isnull().astype('uint8')\n",
    "building.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of buildings missing year by site\n",
    "yr_missing_by_site = pd.DataFrame()\n",
    "yr_missing_by_site['n_values'] = building.groupby('site_id').building_id.count().values\n",
    "yr_missing_by_site['n_missing'] = building.groupby('site_id').missing_year.sum().values\n",
    "yr_missing_by_site['pct_missing'] = (yr_missing_by_site.n_missing / yr_missing_by_site.n_values).round(4) * 100\n",
    "yr_missing_by_site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sites with at least 1 recorded year_built value\n",
    "sites_with_yr_vals = yr_missing_by_site[yr_missing_by_site.pct_missing < 100].index\n",
    "i = 0\n",
    "\n",
    "# Plot these sites\n",
    "fig, ax = plt.subplots(2, 4, figsize=(15, 6))\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "for a in ax:\n",
    "    for b in range(4):\n",
    "        bldg = building[building.site_id == sites_with_yr_vals[i]]\n",
    "        a[b].hist(bldg.year_built)\n",
    "        a[b].set_title('Site {} ({} buildings recorded)'.format(sites_with_yr_vals[i], bldg.shape[0]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half of the sites are missing all of their `year_built` values so the original plan of imputing missing values with site medians won't be able to fill all of the missing data. Let's revise the imputation method for `year_built` a little bit:\n",
    "1. Impute missing values with the site medians\n",
    "2. For sites missing 100% of its values, impute the missing values with the median of similar `primary_use` buildings\n",
    "    - For example, an education building, missing its `year_built` value, will be filled with the median `year_built` of all other education buildings\n",
    "3. If there are still missing values, impute with the site median again\n",
    "4. If there are still missing values, impute with the `primary_use` median again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing year_built values with the building's site's median `year_built` value\n",
    "building['year_built'] = building.groupby('site_id').year_built.transform(lambda y: y.fillna(y.median()))\n",
    "\n",
    "# Check percent missing `year_built` again\n",
    "(yr_missing_by_site['n_values'] - building.groupby('site_id').year_built.count()) * 100 / \\\n",
    "yr_missing_by_site['n_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the other sites with primary_use medians\n",
    "building['year_built'] = building.groupby('primary_use').year_built.transform(lambda y: y.fillna(y.median()))\n",
    "\n",
    "# Fill the rest with the building's site's median\n",
    "building['year_built'] = building.groupby('site_id').year_built.transform(lambda y: y.fillna(y.median())).astype(int)\n",
    "\n",
    "# Check null count again\n",
    "building.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of year-built values\n",
    "building.year_built.plot.hist(bins=40, title='Distribution of year_built')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bldg_missing, yr_missing_by_site, sites_with_yr_vals, i, fig, ax, a, b, bldg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we did impute missing values using the median, most of the values will inevitably be concentrated somewhere around the middle as seen in the barchart above. However, we did manage to spread out the imputation a little bit by grouping by site for some and `primary_use` for others. Otherwise, there would be 1 tall bar in the middle, instead of the 3 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section V: Preprocessing Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new subdirectory in data directory\n",
    "output_path = '../data/from_prep/'\n",
    "udf.mkdir(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save meter data\n",
    "meter.to_pickle(output_path + 'meter.pkl')\n",
    "pd.read_pickle(output_path + 'meter.pkl').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weather data\n",
    "weather.to_pickle(output_path + 'weather.pkl')\n",
    "pd.read_pickle(output_path + 'weather.pkl').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save building data\n",
    "building.to_pickle(output_path + 'building.pkl')\n",
    "pd.read_pickle(output_path + 'building.pkl').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
